{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea:\n",
    "Our solution: LDA + keywords from clusters of BERT based embeddings of noun phrases and verbs :\n",
    "- Each noun phrase and verb in the texts is  transformed to embedding vector using Universal Sentence Encoder (transformer based on BERT)\n",
    "- Embedding vectors from (a) are grouped into clusters with cosign similarity >= 70%\n",
    "- Words/phrases with embedding vectors closest to the centers of resulting clusters form key word/phrase\n",
    "- Each text in the training sample is converted to collection of key-phrases by replacing its noun phrases and verbs with keyword/phrases and deleting other words\n",
    "- LDA is performed on the transformed texts\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "- Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Céspedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil. **Universal Sentence Encoder.** *arXiv:1803.11175, 2018.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clNUUp3MUO2t"
   },
   "source": [
    "# Load data and python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622131163361,
     "user": {
      "displayName": "Tatiana Chebonenko",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwGu7u_TizJ74HmaMD0AtIfiksMdhRYrfZtevXzQ=s64",
      "userId": "10264586744881678851"
     },
     "user_tz": 240
    },
    "id": "7dYQIbH6UO2u"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# topic modeling libraries\n",
    "from gensim import models, corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "# supporting libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import topic_modeling as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape: (33982, 18)\n",
      "df_train.columns: Index(['date', 'author', 'title', 'url', 'section', 'publication',\n",
      "       'first_10_sents', 'list_of_first_10_sents', 'list_of_verb_lemmas',\n",
      "       'noun_phrases', 'list_of_nouns', 'list_of_lemmas', 'ID',\n",
      "       'group_level_1', 'group_level_2', 'group_level_3', 'all_words',\n",
      "       'all_key_words'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(\"./transition_files/df_train_for_LDA.pickle\", 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    df_train = pickle.load(f)\n",
    "\n",
    "print(\"df_train.shape:\", df_train.shape)\n",
    "print(\"df_train.columns:\",df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE American Economic Associations annual conference, held each January, is ostensibly a gigantic teachin, with lots of seminars featuring famous economists. But the threeday event, held this year in San Francisco with 13,000 attending, is also a big jobs fair. More than 500 employersboth universities and companieswere tied up in hotel rooms holding marathon interview sessions with freshly minted PhDs. The ballroom of the Marriott was set aside for a hundred more. It is a gruelling three days for candidates: one exhausted PhD likened it to speeddating. It is also arduous for recruiters. Towards the end of the first day Alan Green and Christopher de Bodisco of Stetson University, a small private college in Florida, review the candidates they have seen so far. They are looking for someone with an interest in health and development. They plan to grill a dozen candidates each day before inviting the most promising ones to visit its campus and meet the rest of the faculty. Upgrade your inbox and get our Daily Dispatch and Editors Picks.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs for testing\n",
    "ind = 10\n",
    "\n",
    "text = df_train['first_10_sents'].iloc[ind]\n",
    "noun_phrases = df_train['noun_phrases'].iloc[ind]\n",
    "list_of_verb_lemmas = df_train['list_of_verb_lemmas'].iloc[ind]\n",
    "all_key_words = df_train['all_key_words'].iloc[ind]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"topics_df_path\": './output/lda_keywords/topics.pickle',\n",
    "                           \"word_embeddings\": './output/lda_keywords/word_embeddings.pickle',\n",
    "                           \"first_dictionary_path\": \"./output/lda_keywords/dictionary1.pickle\",\n",
    "                           \"first_LDA_model_path\": \"./output/lda_keywords/LDA_model1\"\n",
    "                           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cl_number</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cl_size</th>\n",
       "      <th>ID</th>\n",
       "      <th>emb_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.46326, 0.49222, 0.15795, 0.10404, 0.23174,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ascent</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0056534, -0.17881, -0.54648, 0.030204, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climb</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0056534, -0.17881, -0.54648, 0.030204, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defenceunless</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trudge</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.69606, 0.29966, -0.14856, -0.14035, -0.0646...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  cl_number cluster_label  cl_size  ID  \\\n",
       "0           rise       1486          rise       20   0   \n",
       "1         ascent       1486          rise       20   1   \n",
       "2          climb       1486          rise       20   2   \n",
       "3  defenceunless       1486          rise       20   3   \n",
       "4         trudge       1486          rise       20   4   \n",
       "\n",
       "                                          emb_vector  \n",
       "0  [-0.46326, 0.49222, 0.15795, 0.10404, 0.23174,...  \n",
       "1  [0.0056534, -0.17881, -0.54648, 0.030204, 0.10...  \n",
       "2  [0.0056534, -0.17881, -0.54648, 0.030204, 0.10...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.69606, 0.29966, -0.14856, -0.14035, -0.0646...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get clustered words' embedings and cluster names(key words) from train corpus\n",
    "with open(params[\"word_embeddings\"], 'rb') as f:\n",
    "    df_emb = pickle.load(f)\n",
    "\n",
    "columns = [\"emb_\" + str(i) for i in range(300)]\n",
    "df_emb = df_emb.drop(columns=columns)\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76718, 6), (419327, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_emb[df_emb['word'].str.contains(' ') == False]\n",
    "df_tmp.shape, df_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(df_data, column = \"word\", N_batches=500):\n",
    "    #split data into N batches\n",
    "    N = N_batches\n",
    "\n",
    "    part = int(len(df_data)/N)\n",
    "    print(N, \"batches with\", part + 1, column + \"s each\")\n",
    "\n",
    "    #get embeddings for each N words\n",
    "    index = 0\n",
    "    batch_num = 0\n",
    "    list_dfs = []\n",
    "\n",
    "    while index < len(df_data): \n",
    "        df_tmp = df_data.iloc[index : index + part].copy()\n",
    "        df_tmp = df_tmp.reset_index(drop=True)\n",
    "        print (\"Batch number:\", batch_num + 1, \"out of \", N, \"index:\", index)\n",
    "\n",
    "        df_tmp['emb_vector'] = df_tmp[column].apply(lambda w: nlp(w).vector)  \n",
    "\n",
    "        columns = [\"emb_\" + str(i) for i in range(300)]\n",
    "        df_tmp[columns] = np.array(list(df_tmp['emb_vector']))\n",
    "\n",
    "        list_dfs.append(df_tmp)\n",
    "        batch_num = batch_num + 1\n",
    "        index = index + part\n",
    "\n",
    "    #concatinate batches into single dataset\n",
    "    df_emb = pd.concat(list_dfs)\n",
    "\n",
    "    return df_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 batches with 839 words each\n",
      "Batch number: 1 out of  500 index: 0\n",
      "Batch number: 2 out of  500 index: 838\n",
      "Batch number: 3 out of  500 index: 1676\n",
      "Batch number: 4 out of  500 index: 2514\n",
      "Batch number: 5 out of  500 index: 3352\n",
      "Batch number: 6 out of  500 index: 4190\n",
      "Batch number: 7 out of  500 index: 5028\n",
      "Batch number: 8 out of  500 index: 5866\n",
      "Batch number: 9 out of  500 index: 6704\n",
      "Batch number: 10 out of  500 index: 7542\n",
      "Batch number: 11 out of  500 index: 8380\n",
      "Batch number: 12 out of  500 index: 9218\n",
      "Batch number: 13 out of  500 index: 10056\n",
      "Batch number: 14 out of  500 index: 10894\n",
      "Batch number: 15 out of  500 index: 11732\n",
      "Batch number: 16 out of  500 index: 12570\n",
      "Batch number: 17 out of  500 index: 13408\n",
      "Batch number: 18 out of  500 index: 14246\n",
      "Batch number: 19 out of  500 index: 15084\n",
      "Batch number: 20 out of  500 index: 15922\n",
      "Batch number: 21 out of  500 index: 16760\n",
      "Batch number: 22 out of  500 index: 17598\n",
      "Batch number: 23 out of  500 index: 18436\n",
      "Batch number: 24 out of  500 index: 19274\n",
      "Batch number: 25 out of  500 index: 20112\n",
      "Batch number: 26 out of  500 index: 20950\n",
      "Batch number: 27 out of  500 index: 21788\n",
      "Batch number: 28 out of  500 index: 22626\n",
      "Batch number: 29 out of  500 index: 23464\n",
      "Batch number: 30 out of  500 index: 24302\n",
      "Batch number: 31 out of  500 index: 25140\n",
      "Batch number: 32 out of  500 index: 25978\n",
      "Batch number: 33 out of  500 index: 26816\n",
      "Batch number: 34 out of  500 index: 27654\n",
      "Batch number: 35 out of  500 index: 28492\n",
      "Batch number: 36 out of  500 index: 29330\n",
      "Batch number: 37 out of  500 index: 30168\n",
      "Batch number: 38 out of  500 index: 31006\n",
      "Batch number: 39 out of  500 index: 31844\n",
      "Batch number: 40 out of  500 index: 32682\n",
      "Batch number: 41 out of  500 index: 33520\n",
      "Batch number: 42 out of  500 index: 34358\n",
      "Batch number: 43 out of  500 index: 35196\n",
      "Batch number: 44 out of  500 index: 36034\n",
      "Batch number: 45 out of  500 index: 36872\n",
      "Batch number: 46 out of  500 index: 37710\n",
      "Batch number: 47 out of  500 index: 38548\n",
      "Batch number: 48 out of  500 index: 39386\n",
      "Batch number: 49 out of  500 index: 40224\n",
      "Batch number: 50 out of  500 index: 41062\n",
      "Batch number: 51 out of  500 index: 41900\n",
      "Batch number: 52 out of  500 index: 42738\n",
      "Batch number: 53 out of  500 index: 43576\n",
      "Batch number: 54 out of  500 index: 44414\n",
      "Batch number: 55 out of  500 index: 45252\n",
      "Batch number: 56 out of  500 index: 46090\n",
      "Batch number: 57 out of  500 index: 46928\n",
      "Batch number: 58 out of  500 index: 47766\n",
      "Batch number: 59 out of  500 index: 48604\n",
      "Batch number: 60 out of  500 index: 49442\n",
      "Batch number: 61 out of  500 index: 50280\n",
      "Batch number: 62 out of  500 index: 51118\n",
      "Batch number: 63 out of  500 index: 51956\n",
      "Batch number: 64 out of  500 index: 52794\n",
      "Batch number: 65 out of  500 index: 53632\n",
      "Batch number: 66 out of  500 index: 54470\n",
      "Batch number: 67 out of  500 index: 55308\n",
      "Batch number: 68 out of  500 index: 56146\n",
      "Batch number: 69 out of  500 index: 56984\n",
      "Batch number: 70 out of  500 index: 57822\n",
      "Batch number: 71 out of  500 index: 58660\n",
      "Batch number: 72 out of  500 index: 59498\n",
      "Batch number: 73 out of  500 index: 60336\n",
      "Batch number: 74 out of  500 index: 61174\n",
      "Batch number: 75 out of  500 index: 62012\n",
      "Batch number: 76 out of  500 index: 62850\n",
      "Batch number: 77 out of  500 index: 63688\n",
      "Batch number: 78 out of  500 index: 64526\n",
      "Batch number: 79 out of  500 index: 65364\n",
      "Batch number: 80 out of  500 index: 66202\n",
      "Batch number: 81 out of  500 index: 67040\n",
      "Batch number: 82 out of  500 index: 67878\n",
      "Batch number: 83 out of  500 index: 68716\n",
      "Batch number: 84 out of  500 index: 69554\n",
      "Batch number: 85 out of  500 index: 70392\n",
      "Batch number: 86 out of  500 index: 71230\n",
      "Batch number: 87 out of  500 index: 72068\n",
      "Batch number: 88 out of  500 index: 72906\n",
      "Batch number: 89 out of  500 index: 73744\n",
      "Batch number: 90 out of  500 index: 74582\n",
      "Batch number: 91 out of  500 index: 75420\n",
      "Batch number: 92 out of  500 index: 76258\n",
      "Batch number: 93 out of  500 index: 77096\n",
      "Batch number: 94 out of  500 index: 77934\n",
      "Batch number: 95 out of  500 index: 78772\n",
      "Batch number: 96 out of  500 index: 79610\n",
      "Batch number: 97 out of  500 index: 80448\n",
      "Batch number: 98 out of  500 index: 81286\n",
      "Batch number: 99 out of  500 index: 82124\n",
      "Batch number: 100 out of  500 index: 82962\n",
      "Batch number: 101 out of  500 index: 83800\n",
      "Batch number: 102 out of  500 index: 84638\n",
      "Batch number: 103 out of  500 index: 85476\n",
      "Batch number: 104 out of  500 index: 86314\n",
      "Batch number: 105 out of  500 index: 87152\n",
      "Batch number: 106 out of  500 index: 87990\n",
      "Batch number: 107 out of  500 index: 88828\n",
      "Batch number: 108 out of  500 index: 89666\n",
      "Batch number: 109 out of  500 index: 90504\n",
      "Batch number: 110 out of  500 index: 91342\n",
      "Batch number: 111 out of  500 index: 92180\n",
      "Batch number: 112 out of  500 index: 93018\n",
      "Batch number: 113 out of  500 index: 93856\n",
      "Batch number: 114 out of  500 index: 94694\n",
      "Batch number: 115 out of  500 index: 95532\n",
      "Batch number: 116 out of  500 index: 96370\n",
      "Batch number: 117 out of  500 index: 97208\n",
      "Batch number: 118 out of  500 index: 98046\n",
      "Batch number: 119 out of  500 index: 98884\n",
      "Batch number: 120 out of  500 index: 99722\n",
      "Batch number: 121 out of  500 index: 100560\n",
      "Batch number: 122 out of  500 index: 101398\n",
      "Batch number: 123 out of  500 index: 102236\n",
      "Batch number: 124 out of  500 index: 103074\n",
      "Batch number: 125 out of  500 index: 103912\n",
      "Batch number: 126 out of  500 index: 104750\n",
      "Batch number: 127 out of  500 index: 105588\n",
      "Batch number: 128 out of  500 index: 106426\n",
      "Batch number: 129 out of  500 index: 107264\n",
      "Batch number: 130 out of  500 index: 108102\n",
      "Batch number: 131 out of  500 index: 108940\n",
      "Batch number: 132 out of  500 index: 109778\n",
      "Batch number: 133 out of  500 index: 110616\n",
      "Batch number: 134 out of  500 index: 111454\n",
      "Batch number: 135 out of  500 index: 112292\n",
      "Batch number: 136 out of  500 index: 113130\n",
      "Batch number: 137 out of  500 index: 113968\n",
      "Batch number: 138 out of  500 index: 114806\n",
      "Batch number: 139 out of  500 index: 115644\n",
      "Batch number: 140 out of  500 index: 116482\n",
      "Batch number: 141 out of  500 index: 117320\n",
      "Batch number: 142 out of  500 index: 118158\n",
      "Batch number: 143 out of  500 index: 118996\n",
      "Batch number: 144 out of  500 index: 119834\n",
      "Batch number: 145 out of  500 index: 120672\n",
      "Batch number: 146 out of  500 index: 121510\n",
      "Batch number: 147 out of  500 index: 122348\n",
      "Batch number: 148 out of  500 index: 123186\n",
      "Batch number: 149 out of  500 index: 124024\n",
      "Batch number: 150 out of  500 index: 124862\n",
      "Batch number: 151 out of  500 index: 125700\n",
      "Batch number: 152 out of  500 index: 126538\n",
      "Batch number: 153 out of  500 index: 127376\n",
      "Batch number: 154 out of  500 index: 128214\n",
      "Batch number: 155 out of  500 index: 129052\n",
      "Batch number: 156 out of  500 index: 129890\n",
      "Batch number: 157 out of  500 index: 130728\n",
      "Batch number: 158 out of  500 index: 131566\n",
      "Batch number: 159 out of  500 index: 132404\n",
      "Batch number: 160 out of  500 index: 133242\n",
      "Batch number: 161 out of  500 index: 134080\n",
      "Batch number: 162 out of  500 index: 134918\n",
      "Batch number: 163 out of  500 index: 135756\n",
      "Batch number: 164 out of  500 index: 136594\n",
      "Batch number: 165 out of  500 index: 137432\n",
      "Batch number: 166 out of  500 index: 138270\n",
      "Batch number: 167 out of  500 index: 139108\n",
      "Batch number: 168 out of  500 index: 139946\n",
      "Batch number: 169 out of  500 index: 140784\n",
      "Batch number: 170 out of  500 index: 141622\n",
      "Batch number: 171 out of  500 index: 142460\n",
      "Batch number: 172 out of  500 index: 143298\n",
      "Batch number: 173 out of  500 index: 144136\n",
      "Batch number: 174 out of  500 index: 144974\n",
      "Batch number: 175 out of  500 index: 145812\n",
      "Batch number: 176 out of  500 index: 146650\n",
      "Batch number: 177 out of  500 index: 147488\n",
      "Batch number: 178 out of  500 index: 148326\n",
      "Batch number: 179 out of  500 index: 149164\n",
      "Batch number: 180 out of  500 index: 150002\n",
      "Batch number: 181 out of  500 index: 150840\n",
      "Batch number: 182 out of  500 index: 151678\n",
      "Batch number: 183 out of  500 index: 152516\n",
      "Batch number: 184 out of  500 index: 153354\n",
      "Batch number: 185 out of  500 index: 154192\n",
      "Batch number: 186 out of  500 index: 155030\n",
      "Batch number: 187 out of  500 index: 155868\n",
      "Batch number: 188 out of  500 index: 156706\n",
      "Batch number: 189 out of  500 index: 157544\n",
      "Batch number: 190 out of  500 index: 158382\n",
      "Batch number: 191 out of  500 index: 159220\n",
      "Batch number: 192 out of  500 index: 160058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 193 out of  500 index: 160896\n",
      "Batch number: 194 out of  500 index: 161734\n",
      "Batch number: 195 out of  500 index: 162572\n",
      "Batch number: 196 out of  500 index: 163410\n",
      "Batch number: 197 out of  500 index: 164248\n",
      "Batch number: 198 out of  500 index: 165086\n",
      "Batch number: 199 out of  500 index: 165924\n",
      "Batch number: 200 out of  500 index: 166762\n",
      "Batch number: 201 out of  500 index: 167600\n",
      "Batch number: 202 out of  500 index: 168438\n",
      "Batch number: 203 out of  500 index: 169276\n",
      "Batch number: 204 out of  500 index: 170114\n",
      "Batch number: 205 out of  500 index: 170952\n",
      "Batch number: 206 out of  500 index: 171790\n",
      "Batch number: 207 out of  500 index: 172628\n",
      "Batch number: 208 out of  500 index: 173466\n",
      "Batch number: 209 out of  500 index: 174304\n",
      "Batch number: 210 out of  500 index: 175142\n",
      "Batch number: 211 out of  500 index: 175980\n",
      "Batch number: 212 out of  500 index: 176818\n",
      "Batch number: 213 out of  500 index: 177656\n",
      "Batch number: 214 out of  500 index: 178494\n",
      "Batch number: 215 out of  500 index: 179332\n",
      "Batch number: 216 out of  500 index: 180170\n",
      "Batch number: 217 out of  500 index: 181008\n",
      "Batch number: 218 out of  500 index: 181846\n",
      "Batch number: 219 out of  500 index: 182684\n",
      "Batch number: 220 out of  500 index: 183522\n",
      "Batch number: 221 out of  500 index: 184360\n",
      "Batch number: 222 out of  500 index: 185198\n",
      "Batch number: 223 out of  500 index: 186036\n",
      "Batch number: 224 out of  500 index: 186874\n",
      "Batch number: 225 out of  500 index: 187712\n",
      "Batch number: 226 out of  500 index: 188550\n",
      "Batch number: 227 out of  500 index: 189388\n",
      "Batch number: 228 out of  500 index: 190226\n",
      "Batch number: 229 out of  500 index: 191064\n",
      "Batch number: 230 out of  500 index: 191902\n",
      "Batch number: 231 out of  500 index: 192740\n",
      "Batch number: 232 out of  500 index: 193578\n",
      "Batch number: 233 out of  500 index: 194416\n",
      "Batch number: 234 out of  500 index: 195254\n",
      "Batch number: 235 out of  500 index: 196092\n",
      "Batch number: 236 out of  500 index: 196930\n",
      "Batch number: 237 out of  500 index: 197768\n",
      "Batch number: 238 out of  500 index: 198606\n",
      "Batch number: 239 out of  500 index: 199444\n",
      "Batch number: 240 out of  500 index: 200282\n",
      "Batch number: 241 out of  500 index: 201120\n",
      "Batch number: 242 out of  500 index: 201958\n",
      "Batch number: 243 out of  500 index: 202796\n",
      "Batch number: 244 out of  500 index: 203634\n",
      "Batch number: 245 out of  500 index: 204472\n",
      "Batch number: 246 out of  500 index: 205310\n",
      "Batch number: 247 out of  500 index: 206148\n",
      "Batch number: 248 out of  500 index: 206986\n",
      "Batch number: 249 out of  500 index: 207824\n",
      "Batch number: 250 out of  500 index: 208662\n",
      "Batch number: 251 out of  500 index: 209500\n",
      "Batch number: 252 out of  500 index: 210338\n",
      "Batch number: 253 out of  500 index: 211176\n",
      "Batch number: 254 out of  500 index: 212014\n",
      "Batch number: 255 out of  500 index: 212852\n",
      "Batch number: 256 out of  500 index: 213690\n",
      "Batch number: 257 out of  500 index: 214528\n",
      "Batch number: 258 out of  500 index: 215366\n",
      "Batch number: 259 out of  500 index: 216204\n",
      "Batch number: 260 out of  500 index: 217042\n",
      "Batch number: 261 out of  500 index: 217880\n",
      "Batch number: 262 out of  500 index: 218718\n",
      "Batch number: 263 out of  500 index: 219556\n",
      "Batch number: 264 out of  500 index: 220394\n",
      "Batch number: 265 out of  500 index: 221232\n",
      "Batch number: 266 out of  500 index: 222070\n",
      "Batch number: 267 out of  500 index: 222908\n",
      "Batch number: 268 out of  500 index: 223746\n",
      "Batch number: 269 out of  500 index: 224584\n",
      "Batch number: 270 out of  500 index: 225422\n",
      "Batch number: 271 out of  500 index: 226260\n",
      "Batch number: 272 out of  500 index: 227098\n",
      "Batch number: 273 out of  500 index: 227936\n",
      "Batch number: 274 out of  500 index: 228774\n",
      "Batch number: 275 out of  500 index: 229612\n",
      "Batch number: 276 out of  500 index: 230450\n",
      "Batch number: 277 out of  500 index: 231288\n",
      "Batch number: 278 out of  500 index: 232126\n",
      "Batch number: 279 out of  500 index: 232964\n",
      "Batch number: 280 out of  500 index: 233802\n",
      "Batch number: 281 out of  500 index: 234640\n",
      "Batch number: 282 out of  500 index: 235478\n",
      "Batch number: 283 out of  500 index: 236316\n",
      "Batch number: 284 out of  500 index: 237154\n",
      "Batch number: 285 out of  500 index: 237992\n",
      "Batch number: 286 out of  500 index: 238830\n",
      "Batch number: 287 out of  500 index: 239668\n",
      "Batch number: 288 out of  500 index: 240506\n",
      "Batch number: 289 out of  500 index: 241344\n",
      "Batch number: 290 out of  500 index: 242182\n",
      "Batch number: 291 out of  500 index: 243020\n",
      "Batch number: 292 out of  500 index: 243858\n",
      "Batch number: 293 out of  500 index: 244696\n",
      "Batch number: 294 out of  500 index: 245534\n",
      "Batch number: 295 out of  500 index: 246372\n",
      "Batch number: 296 out of  500 index: 247210\n",
      "Batch number: 297 out of  500 index: 248048\n",
      "Batch number: 298 out of  500 index: 248886\n",
      "Batch number: 299 out of  500 index: 249724\n",
      "Batch number: 300 out of  500 index: 250562\n",
      "Batch number: 301 out of  500 index: 251400\n",
      "Batch number: 302 out of  500 index: 252238\n",
      "Batch number: 303 out of  500 index: 253076\n",
      "Batch number: 304 out of  500 index: 253914\n",
      "Batch number: 305 out of  500 index: 254752\n",
      "Batch number: 306 out of  500 index: 255590\n",
      "Batch number: 307 out of  500 index: 256428\n",
      "Batch number: 308 out of  500 index: 257266\n",
      "Batch number: 309 out of  500 index: 258104\n",
      "Batch number: 310 out of  500 index: 258942\n",
      "Batch number: 311 out of  500 index: 259780\n",
      "Batch number: 312 out of  500 index: 260618\n",
      "Batch number: 313 out of  500 index: 261456\n",
      "Batch number: 314 out of  500 index: 262294\n",
      "Batch number: 315 out of  500 index: 263132\n",
      "Batch number: 316 out of  500 index: 263970\n",
      "Batch number: 317 out of  500 index: 264808\n",
      "Batch number: 318 out of  500 index: 265646\n",
      "Batch number: 319 out of  500 index: 266484\n",
      "Batch number: 320 out of  500 index: 267322\n",
      "Batch number: 321 out of  500 index: 268160\n",
      "Batch number: 322 out of  500 index: 268998\n",
      "Batch number: 323 out of  500 index: 269836\n",
      "Batch number: 324 out of  500 index: 270674\n",
      "Batch number: 325 out of  500 index: 271512\n",
      "Batch number: 326 out of  500 index: 272350\n",
      "Batch number: 327 out of  500 index: 273188\n",
      "Batch number: 328 out of  500 index: 274026\n",
      "Batch number: 329 out of  500 index: 274864\n",
      "Batch number: 330 out of  500 index: 275702\n",
      "Batch number: 331 out of  500 index: 276540\n",
      "Batch number: 332 out of  500 index: 277378\n",
      "Batch number: 333 out of  500 index: 278216\n",
      "Batch number: 334 out of  500 index: 279054\n",
      "Batch number: 335 out of  500 index: 279892\n",
      "Batch number: 336 out of  500 index: 280730\n",
      "Batch number: 337 out of  500 index: 281568\n",
      "Batch number: 338 out of  500 index: 282406\n",
      "Batch number: 339 out of  500 index: 283244\n",
      "Batch number: 340 out of  500 index: 284082\n",
      "Batch number: 341 out of  500 index: 284920\n",
      "Batch number: 342 out of  500 index: 285758\n",
      "Batch number: 343 out of  500 index: 286596\n",
      "Batch number: 344 out of  500 index: 287434\n",
      "Batch number: 345 out of  500 index: 288272\n",
      "Batch number: 346 out of  500 index: 289110\n",
      "Batch number: 347 out of  500 index: 289948\n",
      "Batch number: 348 out of  500 index: 290786\n",
      "Batch number: 349 out of  500 index: 291624\n",
      "Batch number: 350 out of  500 index: 292462\n",
      "Batch number: 351 out of  500 index: 293300\n",
      "Batch number: 352 out of  500 index: 294138\n",
      "Batch number: 353 out of  500 index: 294976\n",
      "Batch number: 354 out of  500 index: 295814\n",
      "Batch number: 355 out of  500 index: 296652\n",
      "Batch number: 356 out of  500 index: 297490\n",
      "Batch number: 357 out of  500 index: 298328\n",
      "Batch number: 358 out of  500 index: 299166\n",
      "Batch number: 359 out of  500 index: 300004\n",
      "Batch number: 360 out of  500 index: 300842\n",
      "Batch number: 361 out of  500 index: 301680\n",
      "Batch number: 362 out of  500 index: 302518\n",
      "Batch number: 363 out of  500 index: 303356\n",
      "Batch number: 364 out of  500 index: 304194\n",
      "Batch number: 365 out of  500 index: 305032\n",
      "Batch number: 366 out of  500 index: 305870\n",
      "Batch number: 367 out of  500 index: 306708\n",
      "Batch number: 368 out of  500 index: 307546\n",
      "Batch number: 369 out of  500 index: 308384\n",
      "Batch number: 370 out of  500 index: 309222\n",
      "Batch number: 371 out of  500 index: 310060\n",
      "Batch number: 372 out of  500 index: 310898\n",
      "Batch number: 373 out of  500 index: 311736\n",
      "Batch number: 374 out of  500 index: 312574\n",
      "Batch number: 375 out of  500 index: 313412\n",
      "Batch number: 376 out of  500 index: 314250\n",
      "Batch number: 377 out of  500 index: 315088\n",
      "Batch number: 378 out of  500 index: 315926\n",
      "Batch number: 379 out of  500 index: 316764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 380 out of  500 index: 317602\n",
      "Batch number: 381 out of  500 index: 318440\n",
      "Batch number: 382 out of  500 index: 319278\n",
      "Batch number: 383 out of  500 index: 320116\n",
      "Batch number: 384 out of  500 index: 320954\n",
      "Batch number: 385 out of  500 index: 321792\n",
      "Batch number: 386 out of  500 index: 322630\n",
      "Batch number: 387 out of  500 index: 323468\n",
      "Batch number: 388 out of  500 index: 324306\n",
      "Batch number: 389 out of  500 index: 325144\n",
      "Batch number: 390 out of  500 index: 325982\n",
      "Batch number: 391 out of  500 index: 326820\n",
      "Batch number: 392 out of  500 index: 327658\n",
      "Batch number: 393 out of  500 index: 328496\n",
      "Batch number: 394 out of  500 index: 329334\n",
      "Batch number: 395 out of  500 index: 330172\n",
      "Batch number: 396 out of  500 index: 331010\n",
      "Batch number: 397 out of  500 index: 331848\n",
      "Batch number: 398 out of  500 index: 332686\n",
      "Batch number: 399 out of  500 index: 333524\n",
      "Batch number: 400 out of  500 index: 334362\n",
      "Batch number: 401 out of  500 index: 335200\n",
      "Batch number: 402 out of  500 index: 336038\n",
      "Batch number: 403 out of  500 index: 336876\n",
      "Batch number: 404 out of  500 index: 337714\n",
      "Batch number: 405 out of  500 index: 338552\n",
      "Batch number: 406 out of  500 index: 339390\n",
      "Batch number: 407 out of  500 index: 340228\n",
      "Batch number: 408 out of  500 index: 341066\n",
      "Batch number: 409 out of  500 index: 341904\n",
      "Batch number: 410 out of  500 index: 342742\n",
      "Batch number: 411 out of  500 index: 343580\n",
      "Batch number: 412 out of  500 index: 344418\n",
      "Batch number: 413 out of  500 index: 345256\n",
      "Batch number: 414 out of  500 index: 346094\n",
      "Batch number: 415 out of  500 index: 346932\n",
      "Batch number: 416 out of  500 index: 347770\n",
      "Batch number: 417 out of  500 index: 348608\n",
      "Batch number: 418 out of  500 index: 349446\n",
      "Batch number: 419 out of  500 index: 350284\n",
      "Batch number: 420 out of  500 index: 351122\n",
      "Batch number: 421 out of  500 index: 351960\n",
      "Batch number: 422 out of  500 index: 352798\n",
      "Batch number: 423 out of  500 index: 353636\n",
      "Batch number: 424 out of  500 index: 354474\n",
      "Batch number: 425 out of  500 index: 355312\n",
      "Batch number: 426 out of  500 index: 356150\n",
      "Batch number: 427 out of  500 index: 356988\n",
      "Batch number: 428 out of  500 index: 357826\n",
      "Batch number: 429 out of  500 index: 358664\n",
      "Batch number: 430 out of  500 index: 359502\n",
      "Batch number: 431 out of  500 index: 360340\n",
      "Batch number: 432 out of  500 index: 361178\n",
      "Batch number: 433 out of  500 index: 362016\n",
      "Batch number: 434 out of  500 index: 362854\n",
      "Batch number: 435 out of  500 index: 363692\n",
      "Batch number: 436 out of  500 index: 364530\n",
      "Batch number: 437 out of  500 index: 365368\n",
      "Batch number: 438 out of  500 index: 366206\n",
      "Batch number: 439 out of  500 index: 367044\n",
      "Batch number: 440 out of  500 index: 367882\n",
      "Batch number: 441 out of  500 index: 368720\n",
      "Batch number: 442 out of  500 index: 369558\n",
      "Batch number: 443 out of  500 index: 370396\n",
      "Batch number: 444 out of  500 index: 371234\n",
      "Batch number: 445 out of  500 index: 372072\n",
      "Batch number: 446 out of  500 index: 372910\n",
      "Batch number: 447 out of  500 index: 373748\n",
      "Batch number: 448 out of  500 index: 374586\n",
      "Batch number: 449 out of  500 index: 375424\n",
      "Batch number: 450 out of  500 index: 376262\n",
      "Batch number: 451 out of  500 index: 377100\n",
      "Batch number: 452 out of  500 index: 377938\n",
      "Batch number: 453 out of  500 index: 378776\n",
      "Batch number: 454 out of  500 index: 379614\n",
      "Batch number: 455 out of  500 index: 380452\n",
      "Batch number: 456 out of  500 index: 381290\n",
      "Batch number: 457 out of  500 index: 382128\n",
      "Batch number: 458 out of  500 index: 382966\n",
      "Batch number: 459 out of  500 index: 383804\n",
      "Batch number: 460 out of  500 index: 384642\n",
      "Batch number: 461 out of  500 index: 385480\n",
      "Batch number: 462 out of  500 index: 386318\n",
      "Batch number: 463 out of  500 index: 387156\n",
      "Batch number: 464 out of  500 index: 387994\n",
      "Batch number: 465 out of  500 index: 388832\n",
      "Batch number: 466 out of  500 index: 389670\n",
      "Batch number: 467 out of  500 index: 390508\n",
      "Batch number: 468 out of  500 index: 391346\n",
      "Batch number: 469 out of  500 index: 392184\n",
      "Batch number: 470 out of  500 index: 393022\n",
      "Batch number: 471 out of  500 index: 393860\n",
      "Batch number: 472 out of  500 index: 394698\n",
      "Batch number: 473 out of  500 index: 395536\n",
      "Batch number: 474 out of  500 index: 396374\n",
      "Batch number: 475 out of  500 index: 397212\n",
      "Batch number: 476 out of  500 index: 398050\n",
      "Batch number: 477 out of  500 index: 398888\n",
      "Batch number: 478 out of  500 index: 399726\n",
      "Batch number: 479 out of  500 index: 400564\n",
      "Batch number: 480 out of  500 index: 401402\n",
      "Batch number: 481 out of  500 index: 402240\n",
      "Batch number: 482 out of  500 index: 403078\n",
      "Batch number: 483 out of  500 index: 403916\n",
      "Batch number: 484 out of  500 index: 404754\n",
      "Batch number: 485 out of  500 index: 405592\n",
      "Batch number: 486 out of  500 index: 406430\n",
      "Batch number: 487 out of  500 index: 407268\n",
      "Batch number: 488 out of  500 index: 408106\n",
      "Batch number: 489 out of  500 index: 408944\n",
      "Batch number: 490 out of  500 index: 409782\n",
      "Batch number: 491 out of  500 index: 410620\n",
      "Batch number: 492 out of  500 index: 411458\n",
      "Batch number: 493 out of  500 index: 412296\n",
      "Batch number: 494 out of  500 index: 413134\n",
      "Batch number: 495 out of  500 index: 413972\n",
      "Batch number: 496 out of  500 index: 414810\n",
      "Batch number: 497 out of  500 index: 415648\n",
      "Batch number: 498 out of  500 index: 416486\n",
      "Batch number: 499 out of  500 index: 417324\n",
      "Batch number: 500 out of  500 index: 418162\n",
      "Batch number: 501 out of  500 index: 419000\n",
      "CPU times: user 30min 7s, sys: 3.46 s, total: 30min 10s\n",
      "Wall time: 30min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cl_number</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cl_size</th>\n",
       "      <th>ID</th>\n",
       "      <th>emb_vector</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_290</th>\n",
       "      <th>emb_291</th>\n",
       "      <th>emb_292</th>\n",
       "      <th>emb_293</th>\n",
       "      <th>emb_294</th>\n",
       "      <th>emb_295</th>\n",
       "      <th>emb_296</th>\n",
       "      <th>emb_297</th>\n",
       "      <th>emb_298</th>\n",
       "      <th>emb_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.46326, 0.49222, 0.15795, 0.10404, 0.23174,...</td>\n",
       "      <td>-0.463260</td>\n",
       "      <td>0.49222</td>\n",
       "      <td>0.15795</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539740</td>\n",
       "      <td>0.53004</td>\n",
       "      <td>0.376920</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>-0.17083</td>\n",
       "      <td>0.347810</td>\n",
       "      <td>-0.33974</td>\n",
       "      <td>-0.13493</td>\n",
       "      <td>0.46442</td>\n",
       "      <td>-0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ascent</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0056534, -0.17881, -0.54648, 0.030204, 0.10...</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.17881</td>\n",
       "      <td>-0.54648</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278800</td>\n",
       "      <td>0.25070</td>\n",
       "      <td>0.049530</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-0.28176</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.37623</td>\n",
       "      <td>0.63426</td>\n",
       "      <td>0.51031</td>\n",
       "      <td>-0.278950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climb</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0056534, -0.17881, -0.54648, 0.030204, 0.10...</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.17881</td>\n",
       "      <td>-0.54648</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278800</td>\n",
       "      <td>0.25070</td>\n",
       "      <td>0.049530</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-0.28176</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.37623</td>\n",
       "      <td>0.63426</td>\n",
       "      <td>0.51031</td>\n",
       "      <td>-0.278950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defenceunless</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trudge</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.69606, 0.29966, -0.14856, -0.14035, -0.0646...</td>\n",
       "      <td>0.696060</td>\n",
       "      <td>0.29966</td>\n",
       "      <td>-0.14856</td>\n",
       "      <td>-0.140350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>-0.51598</td>\n",
       "      <td>-0.020915</td>\n",
       "      <td>-0.491170</td>\n",
       "      <td>-0.23036</td>\n",
       "      <td>0.027205</td>\n",
       "      <td>0.79485</td>\n",
       "      <td>0.34300</td>\n",
       "      <td>-0.13924</td>\n",
       "      <td>-0.236760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  cl_number cluster_label  cl_size  ID  \\\n",
       "0           rise       1486          rise       20   0   \n",
       "1         ascent       1486          rise       20   1   \n",
       "2          climb       1486          rise       20   2   \n",
       "3  defenceunless       1486          rise       20   3   \n",
       "4         trudge       1486          rise       20   4   \n",
       "\n",
       "                                          emb_vector     emb_0    emb_1  \\\n",
       "0  [-0.46326, 0.49222, 0.15795, 0.10404, 0.23174,... -0.463260  0.49222   \n",
       "1  [0.0056534, -0.17881, -0.54648, 0.030204, 0.10...  0.005653 -0.17881   \n",
       "2  [0.0056534, -0.17881, -0.54648, 0.030204, 0.10...  0.005653 -0.17881   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000  0.00000   \n",
       "4  [0.69606, 0.29966, -0.14856, -0.14035, -0.0646...  0.696060  0.29966   \n",
       "\n",
       "     emb_2     emb_3  ...   emb_290  emb_291   emb_292   emb_293  emb_294  \\\n",
       "0  0.15795  0.104040  ... -0.539740  0.53004  0.376920 -0.102140 -0.17083   \n",
       "1 -0.54648  0.030204  ... -0.278800  0.25070  0.049530 -0.092657 -0.28176   \n",
       "2 -0.54648  0.030204  ... -0.278800  0.25070  0.049530 -0.092657 -0.28176   \n",
       "3  0.00000  0.000000  ...  0.000000  0.00000  0.000000  0.000000  0.00000   \n",
       "4 -0.14856 -0.140350  ...  0.013825 -0.51598 -0.020915 -0.491170 -0.23036   \n",
       "\n",
       "    emb_295  emb_296  emb_297  emb_298   emb_299  \n",
       "0  0.347810 -0.33974 -0.13493  0.46442 -0.001151  \n",
       "1  0.061938  0.37623  0.63426  0.51031 -0.278950  \n",
       "2  0.061938  0.37623  0.63426  0.51031 -0.278950  \n",
       "3  0.000000  0.00000  0.00000  0.00000  0.000000  \n",
       "4  0.027205  0.79485  0.34300 -0.13924 -0.236760  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_tmp = get_word_embeddings(df_emb, column = \"word\", N_batches=500)   \n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419327, 306)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_tmp['emb_vector']\n",
    "\n",
    "with open('./output/lda_keywords/word_embeddings.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(df_tmp, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(419327, 305)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cl_number</th>\n",
       "      <th>cluster_label</th>\n",
       "      <th>cl_size</th>\n",
       "      <th>ID</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_290</th>\n",
       "      <th>emb_291</th>\n",
       "      <th>emb_292</th>\n",
       "      <th>emb_293</th>\n",
       "      <th>emb_294</th>\n",
       "      <th>emb_295</th>\n",
       "      <th>emb_296</th>\n",
       "      <th>emb_297</th>\n",
       "      <th>emb_298</th>\n",
       "      <th>emb_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.463260</td>\n",
       "      <td>0.49222</td>\n",
       "      <td>0.15795</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>0.231740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539740</td>\n",
       "      <td>0.53004</td>\n",
       "      <td>0.376920</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>-0.17083</td>\n",
       "      <td>0.347810</td>\n",
       "      <td>-0.33974</td>\n",
       "      <td>-0.13493</td>\n",
       "      <td>0.46442</td>\n",
       "      <td>-0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ascent</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.17881</td>\n",
       "      <td>-0.54648</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278800</td>\n",
       "      <td>0.25070</td>\n",
       "      <td>0.049530</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-0.28176</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.37623</td>\n",
       "      <td>0.63426</td>\n",
       "      <td>0.51031</td>\n",
       "      <td>-0.278950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>climb</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.17881</td>\n",
       "      <td>-0.54648</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278800</td>\n",
       "      <td>0.25070</td>\n",
       "      <td>0.049530</td>\n",
       "      <td>-0.092657</td>\n",
       "      <td>-0.28176</td>\n",
       "      <td>0.061938</td>\n",
       "      <td>0.37623</td>\n",
       "      <td>0.63426</td>\n",
       "      <td>0.51031</td>\n",
       "      <td>-0.278950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>defenceunless</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trudge</td>\n",
       "      <td>1486</td>\n",
       "      <td>rise</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.696060</td>\n",
       "      <td>0.29966</td>\n",
       "      <td>-0.14856</td>\n",
       "      <td>-0.140350</td>\n",
       "      <td>-0.064606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013825</td>\n",
       "      <td>-0.51598</td>\n",
       "      <td>-0.020915</td>\n",
       "      <td>-0.491170</td>\n",
       "      <td>-0.23036</td>\n",
       "      <td>0.027205</td>\n",
       "      <td>0.79485</td>\n",
       "      <td>0.34300</td>\n",
       "      <td>-0.13924</td>\n",
       "      <td>-0.236760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  cl_number cluster_label  cl_size  ID     emb_0    emb_1  \\\n",
       "0           rise       1486          rise       20   0 -0.463260  0.49222   \n",
       "1         ascent       1486          rise       20   1  0.005653 -0.17881   \n",
       "2          climb       1486          rise       20   2  0.005653 -0.17881   \n",
       "3  defenceunless       1486          rise       20   3  0.000000  0.00000   \n",
       "4         trudge       1486          rise       20   4  0.696060  0.29966   \n",
       "\n",
       "     emb_2     emb_3     emb_4  ...   emb_290  emb_291   emb_292   emb_293  \\\n",
       "0  0.15795  0.104040  0.231740  ... -0.539740  0.53004  0.376920 -0.102140   \n",
       "1 -0.54648  0.030204  0.102000  ... -0.278800  0.25070  0.049530 -0.092657   \n",
       "2 -0.54648  0.030204  0.102000  ... -0.278800  0.25070  0.049530 -0.092657   \n",
       "3  0.00000  0.000000  0.000000  ...  0.000000  0.00000  0.000000  0.000000   \n",
       "4 -0.14856 -0.140350 -0.064606  ...  0.013825 -0.51598 -0.020915 -0.491170   \n",
       "\n",
       "   emb_294   emb_295  emb_296  emb_297  emb_298   emb_299  \n",
       "0 -0.17083  0.347810 -0.33974 -0.13493  0.46442 -0.001151  \n",
       "1 -0.28176  0.061938  0.37623  0.63426  0.51031 -0.278950  \n",
       "2 -0.28176  0.061938  0.37623  0.63426  0.51031 -0.278950  \n",
       "3  0.00000  0.000000  0.00000  0.00000  0.00000  0.000000  \n",
       "4 -0.23036  0.027205  0.79485  0.34300 -0.13924 -0.236760  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get clustered words' embedings and cluster names(key words) from train corpus\n",
    "with open(params[\"word_embeddings\"], 'rb') as f:\n",
    "    df_emb = pickle.load(f)\n",
    "    \n",
    "\n",
    "print(df_emb.shape)\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american economic associations annual conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gigantic teachin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seminar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous economist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_words\n",
       "0  american economic associations annual conference\n",
       "1                                  gigantic teachin\n",
       "2                                               lot\n",
       "3                                           seminar\n",
       "4                                  famous economist"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract keywords from text\n",
    "NPs_and_Vs = tm.get_NPs_Vs(text)\n",
    "df_text_words = pd.DataFrame(NPs_and_Vs, columns=['text_words'])\n",
    "df_text_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_words</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_290</th>\n",
       "      <th>emb_291</th>\n",
       "      <th>emb_292</th>\n",
       "      <th>emb_293</th>\n",
       "      <th>emb_294</th>\n",
       "      <th>emb_295</th>\n",
       "      <th>emb_296</th>\n",
       "      <th>emb_297</th>\n",
       "      <th>emb_298</th>\n",
       "      <th>emb_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>american economic associations annual conference</td>\n",
       "      <td>-0.265382</td>\n",
       "      <td>0.235481</td>\n",
       "      <td>0.424552</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.136934</td>\n",
       "      <td>-0.132931</td>\n",
       "      <td>0.235726</td>\n",
       "      <td>0.215936</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155192</td>\n",
       "      <td>0.309780</td>\n",
       "      <td>0.037510</td>\n",
       "      <td>-0.177867</td>\n",
       "      <td>-0.002442</td>\n",
       "      <td>-0.136568</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>-0.063044</td>\n",
       "      <td>-0.122973</td>\n",
       "      <td>0.135374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gigantic teachin</td>\n",
       "      <td>-0.380575</td>\n",
       "      <td>-0.493945</td>\n",
       "      <td>-0.122119</td>\n",
       "      <td>-0.133775</td>\n",
       "      <td>0.207755</td>\n",
       "      <td>0.268635</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>-0.193830</td>\n",
       "      <td>-0.010935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>-0.227065</td>\n",
       "      <td>-0.098470</td>\n",
       "      <td>-0.107917</td>\n",
       "      <td>-0.068435</td>\n",
       "      <td>0.474360</td>\n",
       "      <td>-0.011745</td>\n",
       "      <td>0.262315</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>-0.025215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lot</td>\n",
       "      <td>-0.333550</td>\n",
       "      <td>0.496110</td>\n",
       "      <td>-0.278580</td>\n",
       "      <td>-0.205060</td>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.362190</td>\n",
       "      <td>0.055708</td>\n",
       "      <td>-0.268580</td>\n",
       "      <td>0.194180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602150</td>\n",
       "      <td>-0.135000</td>\n",
       "      <td>0.127240</td>\n",
       "      <td>0.174240</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>-0.217800</td>\n",
       "      <td>-0.480430</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seminar</td>\n",
       "      <td>0.050256</td>\n",
       "      <td>0.147340</td>\n",
       "      <td>0.314340</td>\n",
       "      <td>0.311440</td>\n",
       "      <td>0.105810</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>0.321830</td>\n",
       "      <td>-0.340100</td>\n",
       "      <td>-0.098713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391750</td>\n",
       "      <td>-0.031723</td>\n",
       "      <td>0.628290</td>\n",
       "      <td>0.236540</td>\n",
       "      <td>0.054023</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.410360</td>\n",
       "      <td>0.537090</td>\n",
       "      <td>-0.839910</td>\n",
       "      <td>0.303440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous economist</td>\n",
       "      <td>0.043425</td>\n",
       "      <td>0.202752</td>\n",
       "      <td>-0.120916</td>\n",
       "      <td>-0.037330</td>\n",
       "      <td>0.192971</td>\n",
       "      <td>0.266181</td>\n",
       "      <td>0.257690</td>\n",
       "      <td>-0.133566</td>\n",
       "      <td>-0.178595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070634</td>\n",
       "      <td>0.291185</td>\n",
       "      <td>-0.121925</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.206443</td>\n",
       "      <td>-0.189470</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.078929</td>\n",
       "      <td>0.206745</td>\n",
       "      <td>0.107417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_words     emb_0     emb_1  \\\n",
       "0  american economic associations annual conference -0.265382  0.235481   \n",
       "1                                  gigantic teachin -0.380575 -0.493945   \n",
       "2                                               lot -0.333550  0.496110   \n",
       "3                                           seminar  0.050256  0.147340   \n",
       "4                                  famous economist  0.043425  0.202752   \n",
       "\n",
       "      emb_2     emb_3     emb_4     emb_5     emb_6     emb_7     emb_8  ...  \\\n",
       "0  0.424552  0.021250  0.136934 -0.132931  0.235726  0.215936  0.092619  ...   \n",
       "1 -0.122119 -0.133775  0.207755  0.268635  0.110875 -0.193830 -0.010935  ...   \n",
       "2 -0.278580 -0.205060  0.060868  0.362190  0.055708 -0.268580  0.194180  ...   \n",
       "3  0.314340  0.311440  0.105810  0.018469  0.321830 -0.340100 -0.098713  ...   \n",
       "4 -0.120916 -0.037330  0.192971  0.266181  0.257690 -0.133566 -0.178595  ...   \n",
       "\n",
       "    emb_290   emb_291   emb_292   emb_293   emb_294   emb_295   emb_296  \\\n",
       "0 -0.155192  0.309780  0.037510 -0.177867 -0.002442 -0.136568 -0.004954   \n",
       "1  0.125790 -0.227065 -0.098470 -0.107917 -0.068435  0.474360 -0.011745   \n",
       "2 -0.602150 -0.135000  0.127240  0.174240  0.230420  0.409130 -0.217800   \n",
       "3  0.391750 -0.031723  0.628290  0.236540  0.054023  0.027702  0.410360   \n",
       "4  0.070634  0.291185 -0.121925  0.314200  0.206443 -0.189470  0.273500   \n",
       "\n",
       "    emb_297   emb_298   emb_299  \n",
       "0 -0.063044 -0.122973  0.135374  \n",
       "1  0.262315  0.077990 -0.025215  \n",
       "2 -0.480430  0.063816  0.206200  \n",
       "3  0.537090 -0.839910  0.303440  \n",
       "4  0.078929  0.206745  0.107417  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_emb = tm.get_word_embeddings(\n",
    "    df_text_words, column=\"text_words\")\n",
    "df_text_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find closest word in train corpus and get cluster name\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "columns = [\"emb_\" + str(i) for i in range(300)]\n",
    "sim_values = cosine_similarity(df_text_emb[columns], df_emb[columns])\n",
    "max_sim_values = np.max(sim_values, axis=1)\n",
    "df_text_words['take_cluster_name'] = max_sim_values >= 0.7\n",
    "df_text_words['sim_max_index'] = np.argmax(sim_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american_public_health_association gigantic_teachin lot seminar economist twoday_event san_francisco small_business keio_university room marathon_interview_session freshly_minted_phds ballroom marriott tumultuous_week candidate grad_student enlist end day alan_ferguson christopher_bonanos stetson_university small_school florida candidate interest health development dozen_candidate promising_one cdcs_roybal_campus rest mit inbox daily_dispatch editors held involving held going tied schenkerwinkler_holding_swh minting set comparison consider seen looking plan grill invited visit meet upgrade'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_words['keyword'] = df_text_words.apply(\n",
    "        tm.get_keyword, axis=1, args=[df_emb])\n",
    "\n",
    "words_for_LDA = list(df_text_words['keyword'])\n",
    "words_for_LDA = [w.replace(\" \", \"_\") for w in words_for_LDA if len(w) > 0]\n",
    "\n",
    "text = \" \".join(words_for_LDA)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_level_topic</th>\n",
       "      <th>first_level_topic_name</th>\n",
       "      <th>second_level_topic</th>\n",
       "      <th>second_level_topic_name</th>\n",
       "      <th>third_level_topic</th>\n",
       "      <th>third_level_topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hard seltzer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dl1850</td>\n",
       "      <td>0.0.0</td>\n",
       "      <td>kleinman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first_level_topic first_level_topic_name second_level_topic  \\\n",
       "0                  0           hard seltzer                0.0   \n",
       "\n",
       "  second_level_topic_name third_level_topic third_level_topic_name  \n",
       "0                  dl1850             0.0.0               kleinman  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained topics\n",
    "LDA_topics_df_path = params[\"topics_df_path\"]\n",
    "with open(LDA_topics_df_path, 'rb') as f:\n",
    "    df_topics = pickle.load(f)\n",
    "df_topics.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.5373726)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first level\n",
    "first_LDA_dict_path = params[\"first_dictionary_path\"]\n",
    "first_LDA_model_path = params[\"first_LDA_model_path\"]\n",
    "t1, t1_proba = tm.get_top_topic_index(text,\n",
    "                                   params={\"LDA_dictionary_path\": first_LDA_dict_path,\n",
    "                                           \"LDA_model_path\": first_LDA_model_path\n",
    "                                           }\n",
    "                                   )\n",
    "t1, t1_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.9047526)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second level\n",
    "second_LDA_dict_path = first_LDA_dict_path[:-\n",
    "                                           7] + \"_\" + str(t1 + 1) + \".pickle\"\n",
    "second_LDA_model_path = first_LDA_model_path + \"_\" + str(t1 + 1)\n",
    "t2, t2_proba = tm.get_top_topic_index(text,\n",
    "                                   params={\"LDA_dictionary_path\": second_LDA_dict_path,\n",
    "                                           \"LDA_model_path\": second_LDA_model_path\n",
    "                                           }\n",
    "                                   )\n",
    "t2, t2_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.9850626)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third level\n",
    "third_LDA_dict_path = first_LDA_dict_path[:-7] + \\\n",
    "    \"_\" + str(t1 + 1) + \"_\" + str(t2 + 1) + \".pickle\"\n",
    "third_LDA_model_path = first_LDA_model_path + \\\n",
    "    \"_\" + str(t1 + 1) + \"_\" + str(t2 + 1)\n",
    "t3, t3_proba = tm.get_top_topic_index(text,\n",
    "                                   params={\"LDA_dictionary_path\": third_LDA_dict_path,\n",
    "                                           \"LDA_model_path\": third_LDA_model_path\n",
    "                                           }\n",
    "                                   )\n",
    "t3, t3_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boeing ba'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get topic names\n",
    "if t1 == -1:\n",
    "    t1_name = \"misc.\"\n",
    "else:\n",
    "    t1_name = df_topics[df_topics['first_level_topic']\n",
    "                        == t1]['first_level_topic_name'].iloc[0]\n",
    "t1_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rocket'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if t2 == -1:\n",
    "    t2_name = \"misc.\"\n",
    "else:\n",
    "    t2_name = df_topics[df_topics['second_level_topic'] == str(t1) +\n",
    "                        '.' + str(t2)]['second_level_topic_name'].iloc[0]\n",
    "t2_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_level_topic': 1,\n",
       " 'first_level_topic_name': 'daily dispatch',\n",
       " 'first_level_topic_proba': 0.51461256,\n",
       " 'second_level_topic': 1,\n",
       " 'second_level_topic_name': 'inflation',\n",
       " 'second_level_topic_proba': 0.36204377,\n",
       " 'third_level_topic': 1,\n",
       " 'third_level_topic_name': 'alibaba',\n",
       " 'third_level_topic_proba': 0.6143617}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.predict_topics(text,\n",
    "                  params={\"topics_df_path\": './output/lda_keywords/topics.pickle',\n",
    "                          \"word_embeddings\": './output/lda_keywords/word_embeddings.pickle',\n",
    "                          \"first_dictionary_path\": \"./output/lda_keywords/dictionary1.pickle\" ,\n",
    "                          \"first_LDA_model_path\": \"./output/lda_keywords/LDA_model1\"\n",
    "                         }\n",
    "              )  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GLG - 3_step - Assigning Text Groups.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.438px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
